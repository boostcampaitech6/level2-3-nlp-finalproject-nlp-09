dataset:
  type: B # A(감성 대화) or B(공감 대화)
  pathA: ./data/dataset.parquet
  pathB: ./data/*/*/*.json
train_params:
  model_name: EleutherAI/polyglot-ko-5.8b  # CurtisJeon/OrionStarAI-Orion-14B-Base-4bit # EleutherAI/polyglot-ko-5.8b 
  model_4bit: True
  load_adapter: True
  adapter_path: ./trained/EleutherAI-polyglot-ko-5.8b-20240324-172752 # ./trained/best_adapter_e9 # if load_adapter is true, load this adapter to model
  output_dir: ./checkpoints/lora/  # checkpoint save dir
  save_dir: ./trained/  # final model save
  lr_rate: 0.0004
  lr_scheduler_type: cosine
  weight_decay: 0.001
  max_grad_norm: 1.0
  num_train_epochs: 10
  batch_size: 1
  gradient_accumulation_steps: 8
  fp16: True
  bf16: False
  optim: paged_adamw_32bit
  device_map: auto
lora:
  lora_r: 16
  lora_alpha: 32
  lora_dropout: 0.05
  target_modules: ["query_key_value"] # [q_proj, o_proj, k_proj, v_proj, gate_proj, up_proj, down_proj]
quantization:
  use_4bit: True
  bnb_4bit_compute_dtype: float16
  bnb_4bit_quant_type: nf4
  use_nested_quant: False