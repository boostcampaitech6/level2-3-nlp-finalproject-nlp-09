{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import json\n",
    "import sklearn\n",
    "import random\n",
    "import os\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoConfig, Trainer, TrainingArguments\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import pickle as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed:int = 42):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # if use multi-GPU\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RE_Dataset(torch.utils.data.Dataset):\n",
    "  \"\"\" Dataset 구성을 위한 class.\"\"\"\n",
    "  def __init__(self, pair_dataset, labels):\n",
    "    self.pair_dataset = pair_dataset\n",
    "    self.labels = labels\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    item = {key: val[idx].clone().detach() for key, val in self.pair_dataset.items()}\n",
    "    item['labels'] = torch.tensor(self.labels[idx])\n",
    "    return item\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.labels)\n",
    "  \n",
    "def preprocessing_dataset(dataset):\n",
    "  \"\"\" 처음 불러온 csv 파일을 원하는 형태의 DataFrame으로 변경 시켜줍니다.\"\"\"\n",
    "  out_dataset = pd.DataFrame({'url':dataset['url'], 'context':dataset['context'], 'main':dataset['main'], 'detail':dataset['detail']})\n",
    "  return out_dataset\n",
    "\n",
    "def load_data(dataset_dir):\n",
    "  \"\"\" csv 파일을 경로에 맡게 불러 옵니다. \"\"\"\n",
    "  dataset = pd.read_csv(dataset_dir)\n",
    "  dataset = preprocessing_dataset(dataset)\n",
    "  \n",
    "  return dataset\n",
    "\n",
    "def tokenized_dataset(context, tokenizer):\n",
    "  \"\"\" tokenizer에 따라 sentence를 tokenizing 합니다.\"\"\"\n",
    "  tokenized_context = tokenizer(context,return_tensors=\"pt\", padding=True, truncation=True, max_length=256, add_special_tokens=True,)\n",
    "  return tokenized_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def klue_re_micro_f1_for_main(preds, labels):\n",
    "    label_list = ['기쁨', '슬픔', '싫어함(상태)', '분노', '미움(상대방)', '두려움', '수치심', '욕망', '사랑', '중립']\n",
    "    label_indices = list(range(len(label_list)))\n",
    "    return sklearn.metrics.f1_score(labels, preds, average=\"micro\", labels=label_indices) * 100.0\n",
    "\n",
    "def klue_re_auprc_for_main(probs, labels):\n",
    "    labels = np.eye(10)[labels]\n",
    "\n",
    "    score = np.zeros((10,))\n",
    "    for c in range(10):\n",
    "        targets_c = labels.take([c], axis=1).ravel()\n",
    "        preds_c = probs.take([c], axis=1).ravel()\n",
    "        precision, recall, _ = sklearn.metrics.precision_recall_curve(targets_c, preds_c)\n",
    "        score[c] = sklearn.metrics.auc(recall, precision)\n",
    "    return np.average(score) * 100.0\n",
    "\n",
    "def klue_re_micro_f1_for_detail(preds, labels):\n",
    "    label_list = ['만족감', '무기력', '즐거움', '답답함', '타오름', '불쾌', '자랑스러움', '절망', '치사함', '걱정', '부끄러움', '궁금함', '놀람', '아쉬움', '싫증', '공감', '감동', '냉담', '경멸',\n",
    "                  '매력적', '반가움', '불만', '실망', '미안함', '다정함', '공포', '억울함', '난처함', '날카로움', '불신감', '동정(슬픔)', '불편함', '아픔', '고마움', '호감', '귀중함', '기대감', '고통',\n",
    "                  '수치심', '초조함', '원망', '위축감', '후회', '욕심', '시기심', '안정감', '너그러움', '외면', '그리움', '허망', '편안함', '신명남', '비위상함', '반감', '죄책감', '아른거림', '외로움',\n",
    "                  '서먹함', '자신감', '두근거림', '심심함', '갈등', '신뢰감', '열정적인']\n",
    "    label_indices = list(range(len(label_list)))\n",
    "    return sklearn.metrics.f1_score(labels, preds, average=\"micro\", labels=label_indices) * 100.0\n",
    "\n",
    "def klue_re_auprc_for_detail(probs, labels):\n",
    "    labels = np.eye(64)[labels]\n",
    "\n",
    "    score = np.zeros((64,))\n",
    "    for c in range(64):\n",
    "        targets_c = labels.take([c], axis=1).ravel()\n",
    "        preds_c = probs.take([c], axis=1).ravel()\n",
    "        precision, recall, _ = sklearn.metrics.precision_recall_curve(targets_c, preds_c)\n",
    "        score[c] = sklearn.metrics.auc(recall, precision)\n",
    "    return np.average(score) * 100.0\n",
    "\n",
    "def compute_metrics_for_main(pred):\n",
    "  \"\"\" validation을 위한 metrics function \"\"\"\n",
    "  labels = pred.label_ids\n",
    "  preds = pred.predictions.argmax(-1)\n",
    "  probs = pred.predictions\n",
    "\n",
    "  # calculate accuracy using sklearn's function\n",
    "  f1 = klue_re_micro_f1_for_main(preds, labels)\n",
    "  auprc = klue_re_auprc_for_main(probs, labels)\n",
    "  acc = accuracy_score(labels, preds)\n",
    "\n",
    "  return {\n",
    "      'micro f1 score': f1,\n",
    "      'auprc' : auprc,\n",
    "      'accuracy': acc,\n",
    "  }\n",
    "\n",
    "def compute_metrics_for_detail(pred):\n",
    "  \"\"\" validation을 위한 metrics function \"\"\"\n",
    "  labels = pred.label_ids\n",
    "  preds = pred.predictions.argmax(-1)\n",
    "  probs = pred.predictions\n",
    "\n",
    "  # calculate accuracy using sklearn's function\n",
    "  f1 = klue_re_micro_f1_for_detail(preds, labels)\n",
    "  auprc = klue_re_auprc_for_detail(probs, labels)\n",
    "  acc = accuracy_score(labels, preds)\n",
    "\n",
    "  return {\n",
    "      'micro f1 score': f1,\n",
    "      'auprc' : auprc,\n",
    "      'accuracy': acc,\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./detail_to_num.pkl', 'rb') as f:\n",
    "  detail_to_num = pickle.load(f)\n",
    "with open('./num_to_detail.pkl', 'rb') as f:\n",
    "  num_to_detail = pickle.load(f)\n",
    "with open('./main_to_num.pkl', 'rb') as f:\n",
    "  main_to_num = pickle.load(f)\n",
    "with open('./num_to_main.pkl', 'rb') as f:\n",
    "  num_to_main = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = 'google-bert/bert-base-multilingual-uncased'\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>context</th>\n",
       "      <th>main</th>\n",
       "      <th>detail</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://x.com/1hhaa_/status/175373236692813461...</td>\n",
       "      <td>보는동안 너무 행복했고 초콜렛이 너무 먹고싶었고 티모시가 잘생겼고 울어!!하는부분이...</td>\n",
       "      <td>기쁨</td>\n",
       "      <td>만족감</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://x.com/makki_home/status/17552181165049...</td>\n",
       "      <td>어릴 때 가 보고 빕스는 거의 처음인데(기억에 없음) 지금 딸기축제 기간이라 만족스...</td>\n",
       "      <td>기쁨</td>\n",
       "      <td>만족감</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://x.com/302NOW/status/175539358101844788...</td>\n",
       "      <td>미리 계좌로 환전해둔 돈을 해외에서 환전수수료 없이 인출 가능한 트레블로그라는 카드...</td>\n",
       "      <td>기쁨</td>\n",
       "      <td>만족감</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://x.com/Hassen_cos/status/17556459885792...</td>\n",
       "      <td>요즘 번아웃도 자꾸 올라오고 무기력해서 종강하고 교류하기도 버거운 상태가 와부렀으요ㅠㅠ</td>\n",
       "      <td>슬픔</td>\n",
       "      <td>무기력</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://x.com/ssosohae1/status/175618221059468...</td>\n",
       "      <td>크라임씬 장똥민이 범행 도구 찾으려고 화장실 탱크 뒤지는데 거기에 진짜 똥 넣어놓은...</td>\n",
       "      <td>기쁨</td>\n",
       "      <td>즐거움</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url  \\\n",
       "0  https://x.com/1hhaa_/status/175373236692813461...   \n",
       "1  https://x.com/makki_home/status/17552181165049...   \n",
       "2  https://x.com/302NOW/status/175539358101844788...   \n",
       "3  https://x.com/Hassen_cos/status/17556459885792...   \n",
       "4  https://x.com/ssosohae1/status/175618221059468...   \n",
       "\n",
       "                                             context main detail  \n",
       "0  보는동안 너무 행복했고 초콜렛이 너무 먹고싶었고 티모시가 잘생겼고 울어!!하는부분이...   기쁨    만족감  \n",
       "1  어릴 때 가 보고 빕스는 거의 처음인데(기억에 없음) 지금 딸기축제 기간이라 만족스...   기쁨    만족감  \n",
       "2  미리 계좌로 환전해둔 돈을 해외에서 환전수수료 없이 인출 가능한 트레블로그라는 카드...   기쁨    만족감  \n",
       "3  요즘 번아웃도 자꾸 올라오고 무기력해서 종강하고 교류하기도 버거운 상태가 와부렀으요ㅠㅠ    슬픔    무기력  \n",
       "4  크라임씬 장똥민이 범행 도구 찾으려고 화장실 탱크 뒤지는데 거기에 진짜 똥 넣어놓은...   기쁨    즐거움  "
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = load_data('../data/reviewed_emotion_0311.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "main = [main_to_num[value] for value in df.main]\n",
    "detail = [detail_to_num[value] for value in df.detail]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = tokenized_dataset(df['context'].to_list(), tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_test_split 특정 label이 1개 밖에 없어서 stratify 불가\n",
    "# context_train, context_test, main_label_train, main_label_test, detail_label_train, detail_label_test = train_test_split(df['context'], main, detail, test_size=0.3)\n",
    "\n",
    "# RE_main_train_dataset = RE_Dataset(context_train, main_label_train)\n",
    "# RE_main_test_dataset = RE_Dataset(context_test, main_label_test)\n",
    "\n",
    "# RE_detail_train_dataset = RE_Dataset(context_train, detail_label_train)\n",
    "# RE_detail_test_dataset = RE_Dataset(context_test, detail_label_test)\n",
    "\n",
    "# split 없이\n",
    "RE_main_dataset = RE_Dataset(context, main)\n",
    "RE_detail_dataset = RE_Dataset(context, detail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-multilingual-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-multilingual-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(105879, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=64, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_config_for_main =  AutoConfig.from_pretrained(MODEL_NAME)\n",
    "model_config_for_main.num_labels = len(df['main'].unique())\n",
    "\n",
    "model_config_for_detail =  AutoConfig.from_pretrained(MODEL_NAME)\n",
    "model_config_for_detail.num_labels = len(df['detail'].unique())\n",
    "\n",
    "model_for_main =  AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, config=model_config_for_main)\n",
    "model_for_detail =  AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, config=model_config_for_detail)\n",
    "model_for_main.to(device)\n",
    "model_for_detail.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "  output_dir='results_for_main',          # output directory\n",
    "  save_steps=500,                 # model saving step.\n",
    "  num_train_epochs=20,              # total number of training epochs\n",
    "  learning_rate=1e-5,               # learning_rate\n",
    "  per_device_train_batch_size=16,  # batch size per device during training\n",
    "  per_device_eval_batch_size=16,   # batch size for evaluation\n",
    "  warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "  weight_decay=0.01,               # strength of weight decay\n",
    "  evaluation_strategy='steps', # evaluation strategy to adopt during training\n",
    "                              # `no`: No evaluation during training.\n",
    "                              # `steps`: Evaluate every `eval_steps`.\n",
    "                              # `epoch`: Evaluate every end of epoch.\n",
    "  logging_steps=500,              # log saving step.\n",
    "  eval_steps = 500,            # evaluation step.\n",
    "  load_best_model_at_end = True\n",
    "  )\n",
    "trainer_for_main = Trainer(\n",
    "  model=model_for_main,                         # the instantiated 🤗 Transformers model to be trained\n",
    "  args=training_args,                  # training arguments, defined above\n",
    "  train_dataset=RE_main_dataset,         # training dataset\n",
    "  eval_dataset=RE_main_dataset,             # evaluation dataset\n",
    "  compute_metrics=compute_metrics_for_main         # define metrics function\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "  output_dir='results_for_detail',          # output directory\n",
    "  save_steps=500,                 # model saving step.\n",
    "  num_train_epochs=20,              # total number of training epochs\n",
    "  learning_rate=1e-5,               # learning_rate\n",
    "  per_device_train_batch_size=16,  # batch size per device during training\n",
    "  per_device_eval_batch_size=16,   # batch size for evaluation\n",
    "  warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "  weight_decay=0.01,               # strength of weight decay\n",
    "  evaluation_strategy='steps', # evaluation strategy to adopt during training\n",
    "                              # `no`: No evaluation during training.\n",
    "                              # `steps`: Evaluate every `eval_steps`.\n",
    "                              # `epoch`: Evaluate every end of epoch.\n",
    "  logging_steps=500,              # log saving step\n",
    "  eval_steps = 500,            # evaluation step.\n",
    "  load_best_model_at_end = True\n",
    "  )\n",
    "\n",
    "trainer_for_detail = Trainer(\n",
    "  model=model_for_detail,                         # the instantiated 🤗 Transformers model to be trained\n",
    "  args=training_args,                  # training arguments, defined above\n",
    "  train_dataset=RE_detail_dataset,         # training dataset\n",
    "  eval_dataset=RE_detail_dataset,             # evaluation dataset\n",
    "  compute_metrics=compute_metrics_for_detail         # define metrics function\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='398' max='1300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 398/1300 08:29 < 19:19, 0.78 it/s, Epoch 6.11/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer_for_main.train()\n",
    "trainer_for_detail.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# aihub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, tokenized_sent, device):\n",
    "  \"\"\"\n",
    "    test dataset을 DataLoader로 만들어 준 후,\n",
    "    batch_size로 나눠 model이 예측 합니다.\n",
    "  \"\"\"\n",
    "  dataloader = DataLoader(tokenized_sent, batch_size=16, shuffle=False)\n",
    "  model.eval()\n",
    "  output_pred = []\n",
    "  output_prob = []\n",
    "  for i, data in enumerate(dataloader):\n",
    "    with torch.no_grad():\n",
    "      outputs = model(\n",
    "          input_ids=data['input_ids'].to(device),\n",
    "          attention_mask=data['attention_mask'].to(device),\n",
    "          token_type_ids=data['token_type_ids'].to(device)\n",
    "          )\n",
    "    logits = outputs[0]\n",
    "    prob = F.softmax(logits, dim=-1).detach().cpu().numpy()\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    result = np.argmax(logits, axis=-1)\n",
    "\n",
    "    output_pred.append(result)\n",
    "    output_prob.append(prob)\n",
    "  \n",
    "  return np.concatenate(output_pred).tolist(), np.concatenate(output_prob, axis=0).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# active learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/aihub.json') as aihub:\n",
    "  parsed_json = json.load(aihub)\n",
    "  \n",
    "aihub_context = [data['talk']['content']['HS01'] for data in parsed_json]\n",
    "aihub_label = [0] * len(aihub_context)\n",
    "aihub_df = pd.DataFrame({'context':aihub_context, 'label':aihub_label})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "  output_dir='results',          # output directory\n",
    "  save_steps=500,                 # model saving step.\n",
    "  num_train_epochs=20,              # total number of training epochs\n",
    "  learning_rate=1e-5,               # learning_rate\n",
    "  per_device_train_batch_size=16,  # batch size per device during training\n",
    "  per_device_eval_batch_size=16,   # batch size for evaluation\n",
    "  warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "  weight_decay=0.01,               # strength of weight decay\n",
    "  evaluation_strategy='steps', # evaluation strategy to adopt during training\n",
    "                              # `no`: No evaluation during training.\n",
    "                              # `steps`: Evaluate every `eval_steps`.\n",
    "                              # `epoch`: Evaluate every end of epoch.\n",
    "  logging_steps=500,              # log saving step.\n",
    "  eval_steps = 500,            # evaluation step.\n",
    "  load_best_model_at_end = True\n",
    "  )\n",
    "\n",
    "\n",
    "learning_count = 10\n",
    "cut_count = len(aihub_label) // learning_count\n",
    "\n",
    "for n in range(learning_count):\n",
    "  print(f'{n + 1}번째 시작')\n",
    "  aihub_tokenized_context = tokenized_dataset(aihub_df['context'].to_list(), tokenizer)\n",
    "  aihub_label = aihub_df['label'].to_list()\n",
    "\n",
    "  RE_test_dataset = RE_Dataset(aihub_tokenized_context, aihub_label)\n",
    "\n",
    "  print('예측 시작')\n",
    "  main_pred_answer, main_output_prob = inference(model_for_main, RE_test_dataset, device) # model에서 class 추론\n",
    "  detail_pred_answer, detail_output_prob = inference(model_for_detail, RE_test_dataset, device) # model에서 class 추론\n",
    "  print('예측 종료')\n",
    "  # main_pred_answer_label = [num_to_main[value] for value in main_pred_answer]\n",
    "  # detail_pred_answer_label = [num_to_detail[value] for value in detail_pred_answer]\n",
    "  aihub_df['main_pred'] = main_pred_answer\n",
    "  aihub_df['main_prob'] = main_output_prob\n",
    "  aihub_df['main_prob'] = aihub_df['main_prob'].apply(lambda x: max(x))\n",
    "  aihub_df['detail_pred'] = detail_pred_answer\n",
    "  aihub_df['detail_prob'] = detail_output_prob\n",
    "  aihub_df['detail_prob'] = aihub_df['detail_prob'].apply(lambda x: max(x))\n",
    "  aihub_df['prob_mean'] = aihub_df.apply(lambda row: (row['main_prob'] * row['detail_prob']) / 2, axis=1)\n",
    "  aihub_df = aihub_df.sort_values(by='prob_mean', ascending=False)\n",
    "\n",
    "  new_learning_data = aihub_df.iloc[:cut_count]\n",
    "  aihub_df = aihub_df.iloc[cut_count:]\n",
    "\n",
    "  tokenized_context = tokenized_dataset(new_learning_data['context'].to_list(), tokenizer)\n",
    "  NEW_main_train_dataset = RE_Dataset(tokenized_context, new_learning_data['main_pred'].to_list())\n",
    "  NEW_detail_train_dataset = RE_Dataset(tokenized_context, new_learning_data['detail_pred'].to_list())\n",
    "\n",
    "  trainer_for_main = Trainer(\n",
    "    model=model_for_main,                         # the instantiated 🤗 Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=NEW_main_train_dataset,         # training dataset\n",
    "    eval_dataset=RE_main_dataset,             # evaluation dataset\n",
    "    compute_metrics=compute_metrics_for_main         # define metrics function\n",
    "  )\n",
    "\n",
    "  trainer_for_detail = Trainer(\n",
    "    model=model_for_detail,                         # the instantiated 🤗 Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=NEW_detail_train_dataset,         # training dataset\n",
    "    eval_dataset=RE_detail_dataset,             # evaluation dataset\n",
    "    compute_metrics=compute_metrics_for_detail         # define metrics function\n",
    "  )\n",
    "  \n",
    "  print('학습 시작')\n",
    "  trainer_for_main.train()\n",
    "  trainer_for_detail.train()\n",
    "  print('학습 종료')\n",
    "  print(f'{n + 1}번째 종료')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
